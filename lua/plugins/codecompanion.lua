return {
  "olimorris/codecompanion.nvim",
  lazy = false,
  priority = 1,
  dependencies = {
    "nvim-lua/plenary.nvim",
    "nvim-treesitter/nvim-treesitter",
    "hrsh7th/nvim-cmp",
    "nvim-telescope/telescope.nvim",
    "MeanderingProgrammer/render-markdown.nvim",
    { "stevearc/dressing.nvim", opts = {} },
    "franco-ruggeri/codecompanion-spinner.nvim",
    -- "ravitemer/codecompanion-history.nvim",
    -- Optional: Add "rcarriga/nvim-notify" for better "Thinking" notifications
  },
  config = function()
    require("codecompanion").setup {
      display = {
        chat = {
          window = {
            layout = "vertical", -- Options: float, vertical, horizontal
            width = 0.45, -- 0.45 represents 45% of the total editor width
            relative = "editor", -- Ensures the percentage is based on the whole window
            border = "rounded", -- Makes it look much cleaner in NvChad
          },
          auto_scroll = false,
          icons = {
            chat_context = "üìéÔ∏è", -- You can also apply an icon to the fold
          },
          show_intro_message = false,
          fold_context = true,
          show_reasoning = true,
          show_context = true, -- Show context (from slash commands and variables) in the chat buffer?
          show_header_separator = true, -- Show header separators in the chat buffer? Set this to false if you're using an external markdown formatting plugin
          show_settings = true, -- Show LLM settings at the top of the chat buffer?
          show_token_count = true, -- Show the token count for each response?
          show_tools_processing = true, -- Show the loading message when tools are being executed?
          show_token_count = false,
        },
      },
      interactions = {
        chat = {
          adapter = "gemini_cli",
          opts = {
            ---Decorate the user message before it's sent to the LLM
            ---@param message string
            ---@param adapter CodeCompanion.Adapter
            ---@param context table
            ---@return string
            prompt_decorator = function(message, adapter, context)
              return string.format([[<prompt>%s</prompt>]], message)
            end,
          },
          tools = {
            ["cmd_runner"] = {
              opts = {
                allowed_in_yolo_mode = false,
                auto_submit_errors = true, -- Send any errors to the LLM automatically?
                auto_submit_success = true, -- Send any successful output to the LLM automatically?
              },
            },
          },
          roles = {
            ---The header name for the LLM's messages
            ---@type string|fun(adapter: CodeCompanion.Adapter): string
            llm = function(adapter)
              return "Ó∏ç  CodeCompanion (" .. adapter.formatted_name .. ")"
            end,

            ---The header name for your messages
            ---@type string
            user = "ÔäΩ YH",
          },
          variables = {
            ["buffer"] = {
              opts = {
                -- Always sync the buffer by sharing its "diff"
                -- Or choose "all" to share the entire buffer
                default_params = "all",
              },
            },
          },
        },
      },
      extensions = {
        spinner = {},
        -- history = {
        --   enabled = true,
        --   opts = {
        --     -- Keymap to open history from chat buffer (default: gh)
        --     keymap = "gh",
        --     -- Keymap to save the current chat manually (when auto_save is disabled)
        --     save_chat_keymap = "sc",
        --     -- Save all chats by default (disable to save only manually using 'sc')
        --     auto_save = true,
        --     -- Number of days after which chats are automatically deleted (0 to disable)
        --     expiration_days = 0,
        --     -- Picker interface (auto resolved to a valid picker)
        --     picker = "telescope", --- ("telescope", "snacks", "fzf-lua", or "default")
        --     ---Optional filter function to control which chats are shown when browsing
        --     chat_filter = nil, -- function(chat_data) return boolean end
        --     -- Customize picker keymaps (optional)
        --     picker_keymaps = {
        --       rename = { n = "r" },
        --       delete = { n = "d" },
        --       duplicate = { n = "<C-y>" },
        --     },
        --     ---Automatically generate titles for new chats
        --     auto_generate_title = false,
        --     title_generation_opts = {
        --       ---Adapter for generating titles (defaults to current chat adapter)
        --       adapter = nil, -- "copilot"
        --       ---Model for generating titles (defaults to current chat model)
        --       model = nil, -- "gpt-4o"
        --       ---Number of user prompts after which to refresh the title (0 to disable)
        --       refresh_every_n_prompts = 0, -- e.g., 3 to refresh after every 3rd user prompt
        --       ---Maximum number of times to refresh the title (default: 3)
        --       max_refreshes = 3,
        --       format_title = function(original_title)
        --         -- This will display the date and time in your history
        --         -- instead of a summary of the text.
        --         return "Chat: " .. os.date "%Y-%m-%d %H:%M"
        --       end,
        --     },
        --     ---On exiting and entering neovim, loads the last chat on opening chat
        --     continue_last_chat = false,
        --     ---When chat is cleared with `gx` delete the chat from history
        --     delete_on_clearing_chat = false,
        --     ---Directory path to save the chats
        --     dir_to_save = vim.fn.stdpath "data" .. "/codecompanion-history",
        --     ---Enable detailed logging for history extension
        --     enable_logging = false,
        --
        --     -- Summary system
        --     summary = {
        --       -- Keymap to generate summary for current chat (default: "gcs")
        --       create_summary_keymap = "gcs",
        --       -- Keymap to browse summaries (default: "gbs")
        --       browse_summaries_keymap = "gbs",
        --
        --       generation_opts = {
        --         adapter = nil, -- defaults to current chat adapter
        --         model = nil, -- defaults to current chat model
        --         context_size = 90000, -- max tokens that the model supports
        --         include_references = true, -- include slash command content
        --         include_tool_outputs = true, -- include tool execution results
        --         system_prompt = nil, -- custom system prompt (string or function)
        --         format_summary = nil, -- custom function to format generated summary e.g to remove <think/> tags from summary
        --       },
        --     },
        --
        --     -- -- Memory system (requires VectorCode CLI)
        --     -- memory = {
        --     --   -- Automatically index summaries when they are generated
        --     --   auto_create_memories_on_summary_generation = true,
        --     --   -- Path to the VectorCode executable
        --     --   vectorcode_exe = "vectorcode",
        --     --   -- Tool configuration
        --     --   tool_opts = {
        --     --     -- Default number of memories to retrieve
        --     --     default_num = 10,
        --     --   },
        --     --   -- Enable notifications for indexing progress
        --     --   notify = true,
        --     --   -- Index all existing memories on startup
        --     --   -- (requires VectorCode 0.6.12+ for efficient incremental indexing)
        --     --   index_on_startup = false,
        --     -- },
        --   },
        -- },
      },
      opts = {
        log_level = "DEBUG",
        send_code = true,
        use_default_prompt_library = true,
      },

      adapters = {
        acp = {
          gemini_cli = function()
            return require("codecompanion.adapters").extend("gemini_cli", {
              defaults = {
                auth_method = "oauth-personal", -- "oauth-personal"|"gemini-api-key"|"vertex-ai"
              },
              opts = {
                stream = true,
              },
              env = {
                GEMINI_API_KEY = "GEMINI_API_KEY",
                CHECK_UPDATE = "false",
              },
            })
          end,
        },
      },
    }
  end,
}
